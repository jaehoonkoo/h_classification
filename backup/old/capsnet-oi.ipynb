{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### val #######################################################\n",
    "#home_dir = '/home/jkoo/code-tf/hc/tmp/'\n",
    "#home_dir = '/scratch/jkoo/code-tf/hc/tmp/'\n",
    "home_dir = '/mnt-gluster/data/jkoo/code-tf/hc/tmp/'\n",
    "mod = 'val' #str(sys.argv[1])\n",
    "\n",
    "if mod == 'val':\n",
    " model_dir  = 'cnn_finetuned/idx-167' # str(sys.argv[2]) ###############################################\n",
    "#  model_dir  = 'hc_finetuned/idx-168' # str(sys.argv[2]) ###############################################\n",
    " Best_epoch = 15 #int(sys.argv[3]) ###############################################\n",
    "\n",
    " Start = -1\n",
    " End   = Best_epoch\n",
    "\n",
    " def fscore(act,prd):\n",
    "    f1     = []\n",
    "    pr     = []\n",
    "    recall = []\n",
    "    scale = 0.01\n",
    "    End   = int(1/scale)+1\n",
    "    for p in range(0,End):\n",
    "        pr_temp, recall_temp, f1_temp,_ = precision_recall_fscore_support(act, prd>np.float32(p*scale), average='weighted')\n",
    "        pr.append(pr_temp)\n",
    "        recall.append(recall_temp)\n",
    "        f1.append(f1_temp)\n",
    "    F1 = np.array(f1)    \n",
    "    #print (idx_dir)\n",
    "    #print ('Best F1:', np.max(F1), 'with threshold', np.argmax(F1)*scale)\n",
    "    print (np.max(F1),np.float32(np.argmax(F1)*scale))\n",
    "\n",
    " #for ep in range(Start,End):\n",
    " #   idx_dir = model_dir+'-infer-ep-'+ str(ep) +'_val/node/'\n",
    " #   print (idx_dir)\n",
    "\n",
    " for ep in range(Start,End):    \n",
    "    idx_dir = model_dir+'-infer-ep-'+ str(ep) +'_val/node/'\n",
    "    #idx_dir_act = model_dir+'node/'\n",
    "    #idx_dir_prd = model_dir+'val/epoch'+ str(ep) +'/'\n",
    "\n",
    "    act_name = sorted(glob.glob(home_dir+idx_dir+'/act_node_*.npy'))\n",
    "    prd_name = sorted(glob.glob(home_dir+idx_dir+'/prd_*.npy'))    \n",
    "    #act_name = sorted(glob.glob(home_dir+idx_dir_act+'/act_node_*.npy'))\n",
    "    #prd_name = sorted(glob.glob(home_dir+idx_dir_prd+'/prd_*.npy'))    \n",
    "    act = np.load(act_name[0])\n",
    "    prd = np.load(prd_name[0])\n",
    "    for i in range(1,len(act_name)):\n",
    "        act = np.r_[act,np.load(act_name[i])]\n",
    "        prd = np.r_[prd,np.load(prd_name[i])]\n",
    "    fscore(act,prd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir+idx_dir_prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx-157\n",
    "0.7202200079277744 0.62\n",
    "0.7263494156430345 0.62\n",
    "0.7289075019606768 0.61\n",
    "\n",
    "# idx-155\n",
    "0.6950528081723532 0.62\n",
    "0.71035395191315 0.62\n",
    "0.7170758628576938 0.62\n",
    "0.72098653328743 0.62\n",
    "\n",
    "# idx-154\n",
    "0.6743164252414785 0.58\n",
    "0.688536722467147 0.58\n",
    "\n",
    "# idx-153\n",
    "0.704016170487141 0.62\n",
    "0.7158526352252067 0.62\n",
    "0.7217749393936244 0.62\n",
    "0.7253823471729243 0.62\n",
    "0.7261760598073185 0.62\n",
    "\n",
    "# idx-152\n",
    "0.7053572880452884 0.62\n",
    "0.7176788686135307 0.62\n",
    "\n",
    "\n",
    "# idx-151\n",
    "0.718500805843468 0.62\n",
    "0.7279660612755448 0.62\n",
    "0.7307457129510392 0.62\n",
    "0.731822487641629 0.58\n",
    "0.7322419764440721 0.58\n",
    "0.7348688474280004 0.58\n",
    "0.7364695176670245 0.58\n",
    "\n",
    "# runtest 2 \n",
    "0.7184166314958902 0.63\n",
    "0.726938363819136 0.62\n",
    "0.7290980137190183 0.62\n",
    "0.7311485318571879 0.58\n",
    "0.7338203836597205 0.58\n",
    "0.7345946127319414 0.58\n",
    "0.7362335321724572 0.58\n",
    "0.7365544797566254 0.58\n",
    "0.7357307681021401 0.59\n",
    "0.7375278670817424 0.59\n",
    "0.7373402369930431 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data  = [0.7184, 0.7269, 0.7290, 0.7311, 0.7338, 0.7345, 0.7362, 0.7365, 0.7357,0.7375, 0.7373]\n",
    "error = [0.7185, 0.72796, 0.73074, 0.7318,0.7322,0.7348,0.7372,0,0,0,0]\n",
    "\n",
    "x = [i + 1.0 for i in range(len(data))]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.bar(x, data, align=\"center\", edgecolor='black',color='blue', width=0.5)\n",
    "bar1 = ax.bar(x, error, align=\"center\", edgecolor='black',color='blue', width=0.5)\n",
    "# plot = ax.plot(x, error,color='red' )\n",
    "ax.set_xticks(x)\n",
    "# ax.set_xticklabels(('wt', 'N23PP', 'N23PP/PEKN', 'PEKN', 'N23PP/PEKN/L28F'))\n",
    "plt.xlabel('Epoch', fontsize=14, color='black')\n",
    "plt.ylabel('Classification error (%)', fontsize=14, color='black')\n",
    "plt.ylim(0.7, .74)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# city=['None','1st','2nd']\n",
    "city=['1','2','3','4','5','6','7','8','9','10','11']\n",
    "Gender=['run1','run2','run3','run4','run5','FFN-DBN','FFN-DBNOPT']\n",
    "pos = np.arange(len(city))\n",
    "bar_w = 0.25\n",
    "\n",
    "# Index_1=[8.06,8.33,8.10] # 2phase\n",
    "# Index_2=[8.14,8.24,8.09] # DBN+loss\n",
    "# Index_3=[8.25,8.49,8.25] # EL-DBN\n",
    "Index_4=[0.7184, 0.7269, 0.7290, 0.7311, 0.7338, 0.7345, 0.7362, 0.7365, 0.7357,0.7375, 0.7373] # EL-DBNOPT\n",
    "Index_5=[0.7185, 0.72796, 0.73074, 0.7318,0.7322,0.7348,0.7364,0.73729,0.735924,0.7370,0] # FFN-DBN\n",
    "Index_6=[0.72022, 0.72634 ,0.7289,0,0,0,0,0,0,0,0] # FFN-DBN\n",
    "\n",
    "# 0.7202200079277744 0.62\n",
    "# 0.7263494156430345 0.62\n",
    "# Index_6=[7.99,8.21,8.07] # FFN-DBNOPT\n",
    "# Index_7=[7.99,8.46,8.05] # BL\n",
    "# plt.figure(figsize=(6.5,5))\n",
    "# plt.figure(dpi=300)\n",
    "# plt.bar(pos-3*bar_w,Index_1,bar_w,edgecolor='black')\n",
    "# plt.bar(pos-2*bar_w,Index_2,bar_w,edgecolor='black',color='blue')\n",
    "plt.bar(pos-bar_w,Index_4,bar_w,edgecolor='black')\n",
    "plt.bar(pos,Index_5,bar_w,edgecolor='black')\n",
    "plt.bar(pos+bar_w,Index_6,bar_w,edgecolor='black')\n",
    "# plt.bar(pos+2*bar_w,Index_6,bar_w,edgecolor='black',color='green')\n",
    "# plt.bar(pos+3*bar_w,Index_7,bar_w,edgecolor='black',color='lightblue')\n",
    "\n",
    "plt.xticks(pos, city)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Classification error (%)', fontsize=14)\n",
    "# plt.title('Group Barchart - Happiness index across cities By Gender',fontsize=18)\n",
    "# plt.legend(Gender,loc='upper center', bbox_to_anchor=(0.84, 0.54), shadow=True, ncol=1, fontsize=10)\n",
    "plt.legend(Gender,loc='upper center', bbox_to_anchor=(0.85, 0.48), shadow=True, ncol=1, fontsize=9)\n",
    "#plt.legend(Gender,loc=4)\n",
    "plt.ylim(0.7, 0.75)\n",
    "# plt.savefig('ab1_ni.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re produce predictions for path \n",
    "import numpy as np\n",
    "\n",
    "tree = {\n",
    "'0'\t:\t[\t0],\n",
    "'1'\t:\t[\t1],\n",
    "'2'\t:\t[\t0,\t2],\n",
    "'3'\t:\t[\t0,\t3],\n",
    "'4'\t:\t[\t1,\t4],\n",
    "'5'\t:\t[\t1,\t5],\n",
    "'6'\t:\t[\t0,\t2,\t6\t],\n",
    "'7'\t:\t[\t0,\t2,\t7\t],\n",
    "'8'\t:\t[\t0,\t2,\t8\t],\n",
    "'9'\t:\t[\t0,\t2,\t9\t],\n",
    "'10'\t:\t[\t0,\t2,\t10\t],\n",
    "'11'\t:\t[\t0,\t2,\t11\t],\n",
    "'12'\t:\t[\t0,\t2,\t12\t],\n",
    "'13'\t:\t[\t0,\t2,\t13\t],\n",
    "'14'\t:\t[\t0,\t2,\t14\t],\n",
    "'15'\t:\t[\t0,\t2,\t15\t],\n",
    "'16'\t:\t[\t0,\t3,\t16\t],\n",
    "'17'\t:\t[\t0,\t3,\t17\t],\n",
    "'18'\t:\t[\t0,\t3,\t18\t],\n",
    "'19'\t:\t[\t0,\t3,\t19\t],\n",
    "'20'\t:\t[\t0,\t3,\t20\t],\n",
    "'21'\t:\t[\t0,\t3,\t21\t],\n",
    "'22'\t:\t[\t1,\t4,\t22\t],\n",
    "'23'\t:\t[\t1,\t4,\t23\t],\n",
    "'24'\t:\t[\t1,\t4,\t24\t],\n",
    "'25'\t:\t[\t1,\t4,\t25\t],\n",
    "'26'\t:\t[\t1,\t4,\t26\t],\n",
    "'27'\t:\t[\t1,\t4,\t27\t],\n",
    "'28'\t:\t[\t1,\t5,\t28\t],\n",
    "'29'\t:\t[\t1,\t5,\t29\t]}\n",
    "\n",
    "def encoding_prd_path(prd, threshold):\n",
    "    \n",
    "    n_cls = 30\n",
    "    ## create every possible paths from nodes \n",
    "    # actual node is same as actual prd \n",
    "    \n",
    "    prd_path = np.zeros((len(prd),n_cls))\n",
    "    ## turn on \n",
    "    prd = prd > threshold\n",
    "    \n",
    "    ## create label paths from prd \n",
    "    for i in range(len(prd)):\n",
    "        for p in range(n_cls):\n",
    "            act_path_idx = tree[str(p)]\n",
    "            #print (prd[i][act_path_idx])\n",
    "            prd_temp = np.sum(prd[i][act_path_idx])\n",
    "            if len(act_path_idx) == prd_temp:\n",
    "                prd_path[i][p] = 1.0\n",
    "    return prd_path\n",
    "\n",
    "\n",
    "import glob, os, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "###################################################### val #######################################################\n",
    "#home_dir = '/home/jkoo/code-tf/hc/tmp/'\n",
    "#home_dir = '/scratch/jkoo/code-tf/hc/tmp/'\n",
    "home_dir = '/mnt-gluster/data/jkoo/code-tf/hc/tmp/'\n",
    "mod = 'val' #str(sys.argv[1])\n",
    "\n",
    "if mod == 'val':\n",
    " model_dir  = 'cnn_finetuned/idx-142' # str(sys.argv[2]) ###############################################\n",
    " Best_epoch = 4 #int(sys.argv[3]) ###############################################\n",
    "\n",
    " Start = 2 # Best_epoch - 1\n",
    " End   = Start + 1\n",
    "\n",
    " def fscore(act,prd):\n",
    "    f1     = []\n",
    "    pr     = []\n",
    "    recall = []\n",
    "    scale = 0.01\n",
    "    End   = int(1/scale)+1\n",
    "    for p in range(0,End):\n",
    "        threshold = p*scale\n",
    "        prd_path  = encoding_prd_path(prd,threshold)\n",
    "        pr_temp, recall_temp, f1_temp,_ = precision_recall_fscore_support(act, prd_path, average='weighted')        \n",
    "        pr.append(pr_temp)\n",
    "        recall.append(recall_temp)\n",
    "        f1.append(f1_temp)\n",
    "    F1 = np.array(f1)    \n",
    "    #print (idx_dir)\n",
    "    #print ('Best F1:', np.max(F1), 'with threshold', np.argmax(F1)*scale)\n",
    "    print ('Best VAL F1:',np.max(F1),'with threshold',np.argmax(F1)*scale)\n",
    "    print (np.max(F1),np.argmax(F1)*scale)\n",
    "\n",
    " #for ep in range(Start,End):\n",
    " #   idx_dir = model_dir+'-infer-ep-'+ str(ep) +'_val/node/'\n",
    " #   print (idx_dir)\n",
    "\n",
    " for ep in range(Start,End):    \n",
    "    idx_dir = model_dir+'-infer-ep-'+ str(ep) +'_val/node/'\n",
    "    #idx_dir_act = model_dir+'node/'\n",
    "    #idx_dir_prd = model_dir+'val/epoch'+ str(ep) +'/'\n",
    "\n",
    "    act_name = sorted(glob.glob(home_dir+idx_dir+'/act_*.npy'))\n",
    "    prd_name = sorted(glob.glob(home_dir+idx_dir+'/prd_*.npy'))    \n",
    "    #act_name = sorted(glob.glob(home_dir+idx_dir_act+'/act_*.npy'))\n",
    "    #prd_name = sorted(glob.glob(home_dir+idx_dir_prd+'/prd_*.npy'))      \n",
    "    act = np.load(act_name[0])\n",
    "    prd = np.load(prd_name[0])\n",
    "    for i in range(1,len(prd_name)):\n",
    "        act = np.r_[act,np.load(act_name[i])]\n",
    "        #print (np.load(act_name[i]).shape)\n",
    "        prd = np.r_[prd,np.load(prd_name[i])]\n",
    "    print (act.shape,prd.shape)    \n",
    "    fscore(act,prd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://software.intel.com/en-us/articles/understanding-capsule-network-architecture\n",
    "\n",
    "import glob\n",
    "import numpy as  np\n",
    "\n",
    "data_dir = \"/scratch/jkoo/data/open-images/\"                                 ## lj and celje\n",
    "input_dir = data_dir + \"arrays/tr_npy_b256_slim/*.npy\"\n",
    "label_dir = data_dir + \"arrays/labels/tr_list_label_path.npy\"\n",
    "\n",
    "train_filenames = sorted(glob.glob(input_dir))\n",
    "batch_Y     = np.load(label_dir)     ##### s2s\n",
    "\n",
    "#if args.base == \"res\":\n",
    "#   mean_r = 123.68  # ilsvrc1k-123.68  | cifar100-129.30417\n",
    "#   mean_g = 116.779 # ilsvrc1k-116.779 | cifar100-124.06996\n",
    "#   mean_b = 103.935 # ilsvrc1k-103.935 | cifar100-112.43405\n",
    "#else:\n",
    "mean_r = 123.68 # ilsvrc1k-123.68  | cifar100-129.30417\n",
    "mean_g = 116.779 # ilsvrc1k-116.779 | cifar100-124.06996\n",
    "mean_b = 103.935 # ilsvrc1k-103.935 | cifar100-112.43405\n",
    "std_r = 1 # | cifar100-68.14695\n",
    "std_g = 1 # | cifar100-65.37863\n",
    "std_b = 1 # | cifar100-70.40022\n",
    "\n",
    "def preprocessing(batch_x_name): #, cnn_base, server):\n",
    "    #if cnn_base == \"res\" or model_name == \"resnet-50\":\n",
    "    #   if server == 'nu':\n",
    "    #      #batch_x = hkl.load(batch_x_name) ## ilsvrc65\n",
    "    #      batch_x = np.load(batch_x_name)  ## oi \n",
    "    #   else: ## allstate\n",
    "    #      batch_x = np.load(batch_x_name)\n",
    "    #   # batch_x = batch_x.transpose((3,1,2,0)).astype(np.float32)\n",
    "    #else: ## cnn_base == \"vgg\"\n",
    "    #   batch_x = np.load(batch_x_name)\n",
    "    batch_x = np.load(batch_x_name)\n",
    "    batch_x[:,:,:,0] -= mean_r\n",
    "    batch_x[:,:,:,1] -= mean_g\n",
    "    batch_x[:,:,:,2] -= mean_b\n",
    "    batch_x[:,:,:,0] /= std_r\n",
    "    batch_x[:,:,:,1] /= std_g\n",
    "    batch_x[:,:,:,2] /= std_b\n",
    "    return batch_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v1\n",
    "\n",
    "\n",
    "def model_resnet(x,n_class):\n",
    "    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "         net_, end_points_ = resnet_v1.resnet_v1_50(x, n_class, is_training=True)\n",
    "    #var_names_cnn = [i for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='resnet_v1')]\n",
    "\n",
    "    #for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='HC_CONV'):\n",
    "    #     var_names_cnn.append(i)\n",
    "        \n",
    "    # Define 4 blocks\n",
    "    bl1_name_ = 'resnet_v1_50/block1'\n",
    "    bl2_name_ = 'resnet_v1_50/block2'\n",
    "    bl3_name_ = 'resnet_v1_50/block3'\n",
    "    bl4_name_ = 'resnet_v1_50/block4'\n",
    "\n",
    "    blocks = {\n",
    "        'bl1': end_points_[bl1_name_],\n",
    "        'bl2': end_points_[bl2_name_],\n",
    "        'bl3': end_points_[bl3_name_],\n",
    "        'bl4': end_points_[bl4_name_]}\n",
    "\n",
    "    conv1 = blocks['bl1']\n",
    "    conv2 = blocks['bl2']\n",
    "    conv3 = blocks['bl3']\n",
    "    conv4 = blocks['bl4']\n",
    "    return conv4\n",
    "\n",
    "################################################\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Input, Conv2D, Dense\n",
    "from keras.layers import Reshape, Layer, Lambda\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# First, let’s define the Squash function:\n",
    "def squash(output_vector, axis=-1):\n",
    "    norm = tf.reduce_sum(tf.square(output_vector), axis, keep_dims=True)\n",
    "    return output_vector * norm / ((1 + norm) * tf.sqrt(norm + 1.0e-10))\n",
    "\n",
    "# After defining the Squash function, we can define the masking layer:\n",
    "class MaskingLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input, mask = inputs\n",
    "        return K.batch_dot(input, mask, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *_, output_shape = input_shape[0]\n",
    "        return (None, output_shape)\n",
    "    \n",
    "# Now, let’s define the primary Capsule function:\n",
    "def PrimaryCapsule(n_vector, n_channel, n_kernel_size, n_stride, padding='valid'):\n",
    "    def builder(inputs):\n",
    "        output = Conv2D(filters=n_vector * n_channel, kernel_size=n_kernel_size, strides=n_stride, padding=padding)(inputs)\n",
    "        output = Reshape( target_shape=[-1, n_vector], name='primary_capsule_reshape')(output)\n",
    "        return Lambda(squash, name='primary_capsule_squash')(output)\n",
    "    return builder\n",
    "\n",
    "# After that, let’s write the capsule layer class:\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, n_capsule, n_vec, n_routing, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.n_capsule = n_capsule\n",
    "        self.n_vector = n_vec\n",
    "        self.n_routing = n_routing\n",
    "        self.kernel_initializer = initializers.get('he_normal')\n",
    "        self.bias_initializer = initializers.get('zeros')\n",
    "\n",
    "    def build(self, input_shape): # input_shape is a 4D tensor\n",
    "        _, self.input_n_capsule, self.input_n_vector, *_ = input_shape\n",
    "        self.W = self.add_weight(shape=[self.input_n_capsule, self.n_capsule, self.input_n_vector, self.n_vector], initializer=self.kernel_initializer, name='W')\n",
    "        self.bias = self.add_weight(shape=[1, self.input_n_capsule, self.n_capsule, 1, 1], initializer=self.bias_initializer, name='bias', trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_expand = tf.expand_dims(tf.expand_dims(inputs, 2), 2)\n",
    "        input_tiled = tf.tile(input_expand, [1, 1, self.n_capsule, 1, 1])\n",
    "        input_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]), \n",
    "                            elems=input_tiled, initializer=K.zeros( [self.input_n_capsule, self.n_capsule, 1, self.n_vector]))\n",
    "        for i in range(self.n_routing): # routing\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            outputs = squash(tf.reduce_sum( c * input_hat, axis=1, keep_dims=True))\n",
    "            if i != self.n_routing - 1:\n",
    "                self.bias += tf.reduce_sum(input_hat * outputs, axis=-1, keep_dims=True)\n",
    "        return tf.reshape(outputs, [-1, self.n_capsule, self.n_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # output current layer capsules\n",
    "        return (None, self.n_capsule, self.n_vector)\n",
    "\n",
    "# The class below will compute the length of the capsule\n",
    "class LengthLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=-1, keep_dims=False))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *output_shape, _ = input_shape\n",
    "        return tuple(output_shape)\n",
    "\n",
    "# The function below will compute the margin loss:    \n",
    "def margin_loss(y_ground_truth, y_prediction):\n",
    "    _m_plus = 0.9\n",
    "    _m_minus = 0.1\n",
    "    _lambda = 0.5\n",
    "    L = y_ground_truth * tf.square(tf.maximum(0., _m_plus - y_prediction)) + _lambda * ( 1 - y_ground_truth) * tf.square(tf.maximum(0., y_prediction - _m_minus))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "# After defining the different necessary building blocks of the network we can now preprocess the MNIST dataset input for the network:\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "#x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "#y_train = to_categorical(y_train.astype('float32'))\n",
    "#y_test = to_categorical(y_test.astype('float32'))\n",
    "#X = np.concatenate((x_train, x_test), axis=0)\n",
    "#Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# Below are some variables that will represent the shape of the input, number of output classes, and number of routings:\n",
    "input_shape = [224, 224, 3] #[28, 28, 1]\n",
    "n_class     = 30#10\n",
    "n_routing   = 3\n",
    "\n",
    "###########################  Capsule with one CNN layer \n",
    "\n",
    "# Now, let’s create the encoder part of the network:\n",
    "x = Input(shape=input_shape)\n",
    "x = tf.placeholder(\"float\", [None, 224, 224, 3])\n",
    "y = tf.placeholder(\"float\", [None, n_class]) ## allstate s2s\n",
    "\n",
    "conv1 = model_resnet(x,n_class)\n",
    "print (conv1)\n",
    "#conv1 = Conv2D(filters=256, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1')(conv1)\n",
    "primary_capsule = PrimaryCapsule(n_vector=8, n_channel=32, n_kernel_size=7, n_stride=2)(conv1)\n",
    "\n",
    "#conv1 = Conv2D(filters=256, kernel_size=9*8*3, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "#primary_capsule = PrimaryCapsule(n_vector=8, n_channel=32, n_kernel_size=9, n_stride=2)(conv1)\n",
    "digit_capsule = CapsuleLayer( n_capsule=n_class, n_vec=16, n_routing=n_routing, name='digit_capsule')(primary_capsule)\n",
    "output_capsule = LengthLayer(name='output_capsule')(digit_capsule)\n",
    "\n",
    "print (conv1)\n",
    "print (primary_capsule)\n",
    "print (digit_capsule)\n",
    "print (output_capsule)\n",
    "#print (y_test.shape)\n",
    "\n",
    "# Then let’s create the decoder part of the network:\n",
    "#mask_input = Input(shape=(n_class, ))\n",
    "#mask = MaskingLayer()([digit_capsule, mask_input])  # two inputs\n",
    "#dec = Dense(512, activation='relu')(mask)\n",
    "#dec = Dense(1024, activation='relu')(dec)\n",
    "#dec = Dense(784, activation='sigmoid')(dec)\n",
    "#dec = Dense(224*224*3, activation='sigmoid')(dec)\n",
    "#dec = Reshape(input_shape)(dec)\n",
    "\n",
    "# Now let’s create the entire model and compile it:\n",
    "#model = Model([x, mask_input], [output_capsule, dec])\n",
    "#model.compile(optimizer='adam', loss=[ margin_loss, 'mae' ], metrics=[ margin_loss, 'mae', 'accuracy'])\n",
    "\n",
    "#model = Model(x,output_capsule)\n",
    "#model.compile(optimizer='adam', loss=margin_loss, metrics=[ margin_loss, 'accuracy'])\n",
    "\n",
    "#model.summary()\n",
    "#################################################################################\n",
    "learning_rate = 0.1\n",
    "cost = margin_loss(y,output_capsule)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "'''\n",
    "if args.model == \"resnet-50\"  or args.model == \"vgg-16\":\n",
    "   # Define loss and optimizer\n",
    "   #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y)) # multi-label oi\n",
    "   cost = margin_loss(y,output_capsule) # multi-label oi\n",
    "   #pred_cnn = tf.nn.sigmoid(pred)\n",
    "   # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # multi-class\n",
    "   if args.opt == \"adam\":\n",
    "      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "   elif args.opt == \"sgd\":\n",
    "      optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(cost)\n",
    "   else:\n",
    "      print(\"Choose proper Optimizer!!\")\n",
    "      sys.exit()    \n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally, we can train the model for three epochs and find out how it will perform:    \n",
    "model.fit([X], [Y], batch_size=32, epochs=3, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as  np\n",
    "\n",
    "\n",
    "mean_r = 123.68 # ilsvrc1k-123.68  | cifar100-129.30417\n",
    "mean_g = 116.779 # ilsvrc1k-116.779 | cifar100-124.06996\n",
    "mean_b = 103.935 # ilsvrc1k-103.935 | cifar100-112.43405\n",
    "std_r = 1 # | cifar100-68.14695\n",
    "std_g = 1 # | cifar100-65.37863\n",
    "std_b = 1 # | cifar100-70.40022\n",
    "\n",
    "def preprocessing(batch_x_name): #, cnn_base, server):\n",
    "    #if cnn_base == \"res\" or model_name == \"resnet-50\":\n",
    "    #   if server == 'nu':\n",
    "    #      #batch_x = hkl.load(batch_x_name) ## ilsvrc65\n",
    "    #      batch_x = np.load(batch_x_name)  ## oi \n",
    "    #   else: ## allstate\n",
    "    #      batch_x = np.load(batch_x_name)\n",
    "    #   # batch_x = batch_x.transpose((3,1,2,0)).astype(np.float32)\n",
    "    #else: ## cnn_base == \"vgg\"\n",
    "    #   batch_x = np.load(batch_x_name)\n",
    "    batch_x = np.load(batch_x_name)\n",
    "    batch_x[:,:,:,0] -= mean_r\n",
    "    batch_x[:,:,:,1] -= mean_g\n",
    "    batch_x[:,:,:,2] -= mean_b\n",
    "    batch_x[:,:,:,0] /= std_r\n",
    "    batch_x[:,:,:,1] /= std_g\n",
    "    batch_x[:,:,:,2] /= std_b\n",
    "    return batch_x\n",
    "\n",
    "data_dir = \"/scratch/jkoo/data/open-images/\"                                 ## lj and celje\n",
    "input_dir = data_dir + \"arrays/tr_npy_b256_slim/*.npy\"\n",
    "label_dir = data_dir + \"arrays/labels/tr_list_label_path.npy\"\n",
    "input_dir_val = data_dir + \"arrays/val_npy_b256_slim/*.npy\"\n",
    "label_dir_val = data_dir + \"arrays/labels/val_list_label_path.npy\"\n",
    "\n",
    "train_filenames = sorted(glob.glob(input_dir))\n",
    "batch_Y         = np.load(label_dir)     ##### s2s\n",
    "\n",
    "val_filenames = sorted(glob.glob(input_dir_val))\n",
    "batch_Y_val   = np.load(label_dir_val)     ##### s2s\n",
    "\n",
    "Epoch = 5\n",
    "minibatch_n = 256\n",
    "epoch = 0 \n",
    "### training with larger filter size \n",
    "'''\n",
    "while (epoch < Epoch):\n",
    "    # training \n",
    "    for batch_idx in range(len(train_filenames)):\n",
    "        batch_x = preprocessing(train_filenames[batch_idx])#, args.base, args.server)\n",
    "        batch_y = batch_Y[batch_idx*minibatch_n:(batch_idx+1)*minibatch_n]\n",
    "        model.fit(batch_x, batch_y, batch_size=32, epochs=1)\n",
    "        \n",
    "    # validation\n",
    "    acc_val = 0\n",
    "    for batch_idx in range(len(val_filenames)):\n",
    "        batch_x = preprocessing(val_filenames[batch_idx])#, args.base, args.server)\n",
    "        batch_y = batch_Y_val[batch_idx*minibatch_n:(batch_idx+1)*minibatch_n]\n",
    "        results = model.evaluate(batch_x, batch_y, batch_size=32)\n",
    "        acc_val += results[2]\n",
    "        print ('val acc',batch_idx, results[2])\n",
    "    print ('Mean val acc', float(acc_val / len(val_filenames)) )   \n",
    "    epoch +=1\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynew = model.predict(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "# Load data\n",
    "#train_filenames = sorted(glob.glob(input_dir))\n",
    "#val_filenames = sorted(glob.glob(input_dir_val))\n",
    "#te_filenames = sorted(glob.glob(input_dir_te))\n",
    "\n",
    "#print ('=======================================================')\n",
    "#print (len(train_filenames),len(val_filenames),len(te_filenames))\n",
    "#print ('=======================================================')\n",
    "\n",
    "###############################################################################\n",
    "####### hc-s2s\n",
    "\n",
    "# Configuration of the session\n",
    "#session_conf = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "#if args.model == \"hc\":\n",
    "# Launch the graph for hc\n",
    "with tf.Session(config=config) as sess:\n",
    "       #init_fn = slim.assign_from_checkpoint_fn(\n",
    "       #       os.path.join(checkpoints_dir, 'resnet_v1_50.ckpt'),\n",
    "       #       get_init_fn(args.base)) \n",
    "       # initialize sess\n",
    "       sess.run(init)\n",
    "       # Call init_fn\n",
    "       #init_fn(sess)    \n",
    "       n_repeat = 1\n",
    "       batch_size = 32\n",
    "       ## train and validation\n",
    "       for batch_idx in range(len(train_filenames)):\n",
    "            batch_x = preprocessing(train_filenames[batch_idx])#, args.base, args.server)\n",
    "            batch_y_ = batch_Y[batch_idx*len(batch_x):(batch_idx+1)*len(batch_x)]                              ## s2s\n",
    "            batch_y  = batch_y_\n",
    "            # batch_y_path, batch_y, batch_y_target = encode_s2s(pre_path,batch_y_,len(batch_x),n_steps,n_classes,node_added)  ## s2s\n",
    "            # batch_y = y_path node\n",
    "            batch_y_target = None                                                                              ## s2s\n",
    "            repeat = 0\n",
    "            while (repeat < n_repeat):\n",
    "                print(\"..Train..epoch {} -- Repeat {} -- batch: {} / {},\".format(epoch,repeat,batch_idx,len(train_filenames)-1))\n",
    "                for step in range(0,int(len(batch_x)/batch_size)):\n",
    "                    batch_x_temp = batch_x[(step*batch_size):(step+1)*batch_size]\n",
    "                    batch_y_temp = batch_y[(step*batch_size):(step+1)*batch_size]\n",
    "                    #batch_y_path_temp = batch_y_path[(step*batch_size):(step+1)*batch_size]\n",
    "                    sess.run(optimizer, feed_dict={x: batch_x_temp, y: batch_y_temp})\n",
    "                    #loss, acc, pred_s = sess.run([cost, accuracy, pred_cnn], feed_dict={x: batch_x_temp, y: batch_y_temp})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch_x_temp, y: batch_y_temp})\n",
    "                    print (loss)\n",
    "                    p_count, c_count = 0, 0\n",
    "                repeat+=1    \n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256):\n",
    "    a = np.argmax(ynew[i])\n",
    "    if a != 0:\n",
    "        print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fengwang/minimal-capsule\n",
    "\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, Dense, Reshape, Layer, Lambda\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#\n",
    "# The length of the output vector of a capsule is to represent the probability that the entity represented by the capsule\n",
    "# is present in the current unit. A nonlinear squashing function ensures that\n",
    "# - short vectors get shrunk to almost zero length and\n",
    "# - long vectors get shrunk to a length slightly below 1\n",
    "# this function is designed as\n",
    "# v_j = \\frac{||s_j||^2}{1 + ||s_j||^2 } \\frac{s_j}{||s_j||}\n",
    "#\n",
    "def squash(output_vector, axis=-1):\n",
    "    norm = tf.reduce_sum(tf.square(output_vector), axis, keep_dims=True)\n",
    "    return output_vector * norm / ((1 + norm) * tf.sqrt(norm + 1.0e-10))\n",
    "\n",
    "#\n",
    "# This layer takes to input vectors:\n",
    "#   - the first one is the output of the CapsuleLayer, 'n_calss' arrays\n",
    "#   - the ground truth vector, an array with a length of 'n_class', with one of the elements is '1', the rests are '0'\n",
    "#\n",
    "class MaskingLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input, mask = inputs\n",
    "        return K.batch_dot(input, mask, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *_, output_shape = input_shape[0]\n",
    "        return (None, output_shape)\n",
    "\n",
    "\n",
    "#\n",
    "# construct a conv layer, then reshape and apply squash operation\n",
    "#\n",
    "def PrimaryCapsule(n_vector, n_channel, n_kernel_size, n_stride, padding='valid'):\n",
    "    def builder(inputs):\n",
    "        output = Conv2D(filters=n_vector * n_channel, kernel_size=n_kernel_size, strides=n_stride, padding=padding)(inputs)\n",
    "        output = Reshape( target_shape=[-1, n_vector], name='primary_capsule_reshape')(output)\n",
    "        return Lambda(squash, name='primary_capsule_squash')(output)\n",
    "    return builder\n",
    "\n",
    "#\n",
    "# Traditional Neural Network          Capsule\n",
    "# scalar in scalar out       -->>     vector in vector out/matrix in matrix out\n",
    "# back propagation update    -->>     routing update\n",
    "#\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, n_capsule, n_vec, n_routing, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.n_capsule = n_capsule\n",
    "        self.n_vector = n_vec\n",
    "        self.n_routing = n_routing\n",
    "        self.kernel_initializer = initializers.get('he_normal')\n",
    "        self.bias_initializer = initializers.get('zeros')\n",
    "\n",
    "    def build(self, input_shape): # input_shape is a 4D tensor\n",
    "        _, self.input_n_capsule, self.input_n_vector, *_ = input_shape\n",
    "        self.W = self.add_weight(shape=[self.input_n_capsule, self.n_capsule, self.input_n_vector, self.n_vector], initializer=self.kernel_initializer, name='W')\n",
    "        self.bias = self.add_weight(shape=[1, self.input_n_capsule, self.n_capsule, 1, 1], initializer=self.bias_initializer, name='bias', trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_expand = tf.expand_dims(tf.expand_dims(inputs, 2), 2)\n",
    "        input_tiled = tf.tile(input_expand, [1, 1, self.n_capsule, 1, 1])\n",
    "        input_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]), elems=input_tiled, initializer=K.zeros( [self.input_n_capsule, self.n_capsule, 1, self.n_vector]))\n",
    "        for i in range(self.n_routing): # routing\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            outputs = squash(tf.reduce_sum( c * input_hat, axis=1, keep_dims=True))\n",
    "            if i != self.n_routing - 1:\n",
    "                self.bias += tf.reduce_sum(input_hat * outputs, axis=-1, keep_dims=True)\n",
    "        return tf.reshape(outputs, [-1, self.n_capsule, self.n_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # output current layer capsules\n",
    "        return (None, self.n_capsule, self.n_vector)\n",
    "\n",
    "#\n",
    "# This layer takes 'n_class' arrays as input, outputs an array of size 'n_class',\n",
    "# each eleemnt in the output array represent the possibility,\n",
    "# i.e., the last layer in Figure 2.\n",
    "#\n",
    "class LengthLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=-1, keep_dims=False))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *output_shape, _ = input_shape\n",
    "        return tuple(output_shape)\n",
    "\n",
    "\n",
    "#\n",
    "# margin loss is employed to measure the accuracy of the capsule net,\n",
    "# in the code below, mean absolute error is used to measure the accuracy of the reconstructed image\n",
    "#\n",
    "def margin_loss(y_ground_truth, y_prediction):\n",
    "    _m_plus = 0.9\n",
    "    _m_minus = 0.1\n",
    "    _lambda = 0.5\n",
    "    L = y_ground_truth * tf.square(tf.maximum(0., _m_plus - y_prediction)) + _lambda * ( 1 - y_ground_truth) * tf.square(tf.maximum(0., y_prediction - _m_minus))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # preprocess MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    X = np.concatenate((x_train, x_test), axis=0)\n",
    "    Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "    # make model\n",
    "    input_shape = [28, 28, 1]\n",
    "    n_class = 10\n",
    "    n_routing = 3\n",
    "\n",
    "    # encoder\n",
    "    x = Input(shape=input_shape)\n",
    "    conv1 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    primary_capsule = PrimaryCapsule( n_vector=8, n_channel=32, n_kernel_size=9, n_stride=2)(conv1)\n",
    "    digit_capsule = CapsuleLayer( n_capsule=n_class, n_vec=16, n_routing=n_routing, name='digit_capsule')(primary_capsule)\n",
    "    output_capsule = LengthLayer(name='output_capsule')(digit_capsule)\n",
    "\n",
    "    # decoder\n",
    "    mask_input = Input(shape=(n_class, ))\n",
    "    mask = MaskingLayer()([digit_capsule, mask_input])  # two inputs\n",
    "    dec = Dense(512, activation='relu')(mask)\n",
    "    dec = Dense(1024, activation='relu')(dec)\n",
    "    dec = Dense(784, activation='sigmoid')(dec)\n",
    "    dec = Reshape(input_shape)(dec)\n",
    "\n",
    "    model = Model([x, mask_input], [output_capsule, dec])\n",
    "    plot_model(model, 'capsule.png', show_shapes=True, rankdir='TB')\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss=[ margin_loss, 'mae' ], metrics=[ margin_loss, 'mae'])\n",
    "\n",
    "    # train capsule model\n",
    "    model.fit([X, Y], [Y, X], batch_size=128, epochs=50, validation_split=0.2)\n",
    "    #model.save_weights('capsule_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://search.daum.net/search?nil_suggest=sugsch&w=tot&DA=GIQ&sq=Boilerplate+code&o=1&sugo=1&q=Boilerplate+code\n",
    "import gc\n",
    "import os\n",
    "import nltk\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, words_dict):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        if hasattr(sentence, \"decode\"):\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        result = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = len(words_dict)\n",
    "            word_index = words_dict[word]\n",
    "            result.append(word_index)\n",
    "        tokenized_sentences.append(result)\n",
    "    return tokenized_sentences, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_list(file_path):\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    f = open(file_path)\n",
    "\n",
    "    for index, line in enumerate(f):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            continue\n",
    "        embedding_list.append(coefs)\n",
    "        embedding_word_dict[word] = len(embedding_word_dict)\n",
    "    f.close()\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n",
    "    cleared_embedding_list = []\n",
    "    cleared_embedding_word_dict = {}\n",
    "\n",
    "    for word in words_dict:\n",
    "        if word not in embedding_word_dict:\n",
    "            continue\n",
    "        word_id = embedding_word_dict[word]\n",
    "        row = embedding_list[word_id]\n",
    "        cleared_embedding_list.append(row)\n",
    "        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n",
    "\n",
    "    return cleared_embedding_list, cleared_embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokenized_sentences, words_list, embedding_word_dict, sentences_length):\n",
    "    words_train = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            word = words_list[word_index]\n",
    "            word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n",
    "            current_words.append(word_id)\n",
    "\n",
    "        if len(current_words) >= sentences_length:\n",
    "            current_words = current_words[:sentences_length]\n",
    "        else:\n",
    "            current_words += [len(embedding_word_dict) - 1] * (sentences_length - len(current_words))\n",
    "        words_train.append(current_words)\n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.3\n",
    "rate_drop_dense = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, sequence_length, dropout_rate, recurrent_units, dense_size):\n",
    "    input1 = Input(shape=(sequence_length,))\n",
    "    embed_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(1, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    num_labels = train_y.shape[1]\n",
    "    patience = 5\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(num_labels):\n",
    "            loss = log_loss(val_y[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_labels\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == patience:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(X, y, X_test, fold_count, batch_size, get_model_func):\n",
    "    print(\"=\"*75)\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    result_path = \"predictions\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = np.array(X[fold_start:fold_end])\n",
    "        val_y = np.array(y[fold_start:fold_end])\n",
    "\n",
    "        model = _train_model(get_model_func(), batch_size, train_x, train_y, val_x, val_y)\n",
    "        train_predicts_path = os.path.join(result_path, \"train_predicts{0}.npy\".format(fold_id))\n",
    "        test_predicts_path = os.path.join(result_path, \"test_predicts{0}.npy\".format(fold_id))\n",
    "        train_predicts = model.predict(X, batch_size=512, verbose=1)\n",
    "        test_predicts = model.predict(X_test, batch_size=512, verbose=1)\n",
    "        np.save(train_predicts_path, train_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"../input/donorschooseorg-preprocessed-data/train_preprocessed.csv\"\n",
    "train_file_path = \"../input/donorschooseorg-preprocessed-data/train_small.csv\"\n",
    "\n",
    "# test_file_path = \"../input/donorschooseorg-preprocessed-data/test_preprocessed.csv\"\n",
    "test_file_path = \"../input/donorschooseorg-preprocessed-data/test_small.csv\"\n",
    "\n",
    "# embedding_path = \"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\n",
    "embedding_path = \"../input/donorschooseorg-preprocessed-data/embeddings_small.vec\"\n",
    "\n",
    "batch_size = 128 # 256\n",
    "recurrent_units = 16 # 64\n",
    "dropout_rate = 0.3 \n",
    "dense_size = 8 # 32\n",
    "sentences_length = 10 # 300\n",
    "fold_count = 2 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcdl0tf3111",
   "language": "python",
   "name": "wcdl0tf3111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

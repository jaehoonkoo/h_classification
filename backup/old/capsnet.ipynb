{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://software.intel.com/en-us/articles/understanding-capsule-network-architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5000d0e6b247>:16: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-1-5000d0e6b247>:59: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "Tensor(\"HC_CONV/digit_capsule/Reshape:0\", shape=(?, 10, 16), dtype=float32)\n",
      "Tensor(\"HC_CONV/output_capsule/Sqrt:0\", shape=(?, 10), dtype=float32)\n",
      "(10000, 10)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primary_capsule_reshape (Reshap (None, 1152, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "primary_capsule_squash (Lambda) (None, 1152, 8)      0           primary_capsule_reshape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "digit_capsule (CapsuleLayer)    (None, 10, 16)       1486080     primary_capsule_squash[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_layer_1 (MaskingLayer)  (None, 16)           0           digit_capsule[0][0]              \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          8704        masking_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 784)          803600      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_capsule (LengthLayer)    (None, 10)           0           digit_capsule[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 1)    0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n",
      "[<tf.Variable 'HC_CONV/conv1/kernel:0' shape=(9, 9, 1, 256) dtype=float32_ref>, <tf.Variable 'HC_CONV/conv1/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'HC_CONV/conv2d_1/kernel:0' shape=(9, 9, 256, 256) dtype=float32_ref>, <tf.Variable 'HC_CONV/conv2d_1/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'HC_CONV/digit_capsule/W:0' shape=(1152, 10, 8, 16) dtype=float32_ref>, <tf.Variable 'HC_CONV/digit_capsule/bias:0' shape=(1, 1152, 10, 1, 1) dtype=float32_ref>, <tf.Variable 'HC_CONV/digit_capsule/Variable:0' shape=(1152, 10, 1, 16) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Input, Conv2D, Dense\n",
    "from keras.layers import Reshape, Layer, Lambda\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# First, let’s define the Squash function:\n",
    "def squash(output_vector, axis=-1):\n",
    "    norm = tf.reduce_sum(tf.square(output_vector), axis, keep_dims=True)\n",
    "    return output_vector * norm / ((1 + norm) * tf.sqrt(norm + 1.0e-10))\n",
    "\n",
    "# After defining the Squash function, we can define the masking layer:\n",
    "class MaskingLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input, mask = inputs\n",
    "        return K.batch_dot(input, mask, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *_, output_shape = input_shape[0]\n",
    "        return (None, output_shape)\n",
    "    \n",
    "# Now, let’s define the primary Capsule function:\n",
    "def PrimaryCapsule(n_vector, n_channel, n_kernel_size, n_stride, padding='valid'):\n",
    "    def builder(inputs):\n",
    "        output = Conv2D(filters=n_vector * n_channel, kernel_size=n_kernel_size, strides=n_stride, padding=padding)(inputs)\n",
    "        output = Reshape( target_shape=[-1, n_vector], name='primary_capsule_reshape')(output)\n",
    "        return Lambda(squash, name='primary_capsule_squash')(output)\n",
    "    return builder\n",
    "\n",
    "# After that, let’s write the capsule layer class:\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, n_capsule, n_vec, n_routing, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.n_capsule = n_capsule\n",
    "        self.n_vector = n_vec\n",
    "        self.n_routing = n_routing\n",
    "        self.kernel_initializer = initializers.get('he_normal')\n",
    "        self.bias_initializer = initializers.get('zeros')\n",
    "\n",
    "    def build(self, input_shape): # input_shape is a 4D tensor\n",
    "        _, self.input_n_capsule, self.input_n_vector, *_ = input_shape\n",
    "        self.W = self.add_weight(shape=[self.input_n_capsule, self.n_capsule, self.input_n_vector, self.n_vector], initializer=self.kernel_initializer, name='W')\n",
    "        self.bias = self.add_weight(shape=[1, self.input_n_capsule, self.n_capsule, 1, 1], initializer=self.bias_initializer, name='bias', trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_expand = tf.expand_dims(tf.expand_dims(inputs, 2), 2)\n",
    "        input_tiled = tf.tile(input_expand, [1, 1, self.n_capsule, 1, 1])\n",
    "        input_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]), \n",
    "                            elems=input_tiled, initializer=K.zeros( [self.input_n_capsule, self.n_capsule, 1, self.n_vector]))\n",
    "        for i in range(self.n_routing): # routing\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            outputs = squash(tf.reduce_sum( c * input_hat, axis=1, keep_dims=True))\n",
    "            if i != self.n_routing - 1:\n",
    "                self.bias += tf.reduce_sum(input_hat * outputs, axis=-1, keep_dims=True)\n",
    "        return tf.reshape(outputs, [-1, self.n_capsule, self.n_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # output current layer capsules\n",
    "        return (None, self.n_capsule, self.n_vector)\n",
    "\n",
    "# The class below will compute the length of the capsule\n",
    "class LengthLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=-1, keep_dims=False))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *output_shape, _ = input_shape\n",
    "        return tuple(output_shape)\n",
    "\n",
    "# The function below will compute the margin loss:    \n",
    "def margin_loss(y_ground_truth, y_prediction):\n",
    "    _m_plus = 0.9\n",
    "    _m_minus = 0.1\n",
    "    _lambda = 0.5\n",
    "    L = y_ground_truth * tf.square(tf.maximum(0., _m_plus - y_prediction)) + _lambda * ( 1 - y_ground_truth) * tf.square(tf.maximum(0., y_prediction - _m_minus))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "# After defining the different necessary building blocks of the network we can now preprocess the MNIST dataset input for the network:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train.astype('float32'))\n",
    "y_test = to_categorical(y_test.astype('float32'))\n",
    "X = np.concatenate((x_train, x_test), axis=0)\n",
    "Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# Below are some variables that will represent the shape of the input, number of output classes, and number of routings:\n",
    "input_shape = [28, 28, 1]\n",
    "n_class = 10\n",
    "n_routing = 3\n",
    "\n",
    "# Now, let’s create the encoder part of the network:\n",
    "\n",
    "with tf.name_scope('HC_CONV'):\n",
    " x = Input(shape=input_shape)\n",
    " conv1 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    " primary_capsule = PrimaryCapsule( n_vector=8, n_channel=32, n_kernel_size=9, n_stride=2)(conv1)\n",
    " digit_capsule = CapsuleLayer( n_capsule=n_class, n_vec=16, n_routing=n_routing, name='digit_capsule')(primary_capsule)\n",
    " output_capsule = LengthLayer(name='output_capsule')(digit_capsule)\n",
    "\n",
    " print (digit_capsule)\n",
    " print (output_capsule)\n",
    " print (y_test.shape)\n",
    "\n",
    "# Then let’s create the decoder part of the network:\n",
    "mask_input = Input(shape=(n_class, ))\n",
    "mask = MaskingLayer()([digit_capsule, mask_input])  # two inputs\n",
    "dec = Dense(512, activation='relu')(mask)\n",
    "dec = Dense(1024, activation='relu')(dec)\n",
    "dec = Dense(784, activation='sigmoid')(dec)\n",
    "dec = Reshape(input_shape)(dec)\n",
    "\n",
    "# Now let’s create the entire model and compile it:\n",
    "model = Model([x, mask_input], [output_capsule, dec])\n",
    "model.compile(optimizer='adam', loss=[ margin_loss, 'mae' ], metrics=[ margin_loss, 'mae', 'accuracy'])\n",
    "\n",
    "model.summary()\n",
    "    \n",
    "var_names_cnn = [i for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='HC_CONV')]    \n",
    "print (var_names_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can train the model for three epochs and find out how it will perform:    \n",
    "model.fit([X, Y], [Y, X], batch_size=128, epochs=3, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fengwang/minimal-capsule\n",
    "\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, Dense, Reshape, Layer, Lambda\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#\n",
    "# The length of the output vector of a capsule is to represent the probability that the entity represented by the capsule\n",
    "# is present in the current unit. A nonlinear squashing function ensures that\n",
    "# - short vectors get shrunk to almost zero length and\n",
    "# - long vectors get shrunk to a length slightly below 1\n",
    "# this function is designed as\n",
    "# v_j = \\frac{||s_j||^2}{1 + ||s_j||^2 } \\frac{s_j}{||s_j||}\n",
    "#\n",
    "def squash(output_vector, axis=-1):\n",
    "    norm = tf.reduce_sum(tf.square(output_vector), axis, keep_dims=True)\n",
    "    return output_vector * norm / ((1 + norm) * tf.sqrt(norm + 1.0e-10))\n",
    "\n",
    "#\n",
    "# This layer takes to input vectors:\n",
    "#   - the first one is the output of the CapsuleLayer, 'n_calss' arrays\n",
    "#   - the ground truth vector, an array with a length of 'n_class', with one of the elements is '1', the rests are '0'\n",
    "#\n",
    "class MaskingLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input, mask = inputs\n",
    "        return K.batch_dot(input, mask, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *_, output_shape = input_shape[0]\n",
    "        return (None, output_shape)\n",
    "\n",
    "\n",
    "#\n",
    "# construct a conv layer, then reshape and apply squash operation\n",
    "#\n",
    "def PrimaryCapsule(n_vector, n_channel, n_kernel_size, n_stride, padding='valid'):\n",
    "    def builder(inputs):\n",
    "        output = Conv2D(filters=n_vector * n_channel, kernel_size=n_kernel_size, strides=n_stride, padding=padding)(inputs)\n",
    "        output = Reshape( target_shape=[-1, n_vector], name='primary_capsule_reshape')(output)\n",
    "        return Lambda(squash, name='primary_capsule_squash')(output)\n",
    "    return builder\n",
    "\n",
    "#\n",
    "# Traditional Neural Network          Capsule\n",
    "# scalar in scalar out       -->>     vector in vector out/matrix in matrix out\n",
    "# back propagation update    -->>     routing update\n",
    "#\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, n_capsule, n_vec, n_routing, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.n_capsule = n_capsule\n",
    "        self.n_vector = n_vec\n",
    "        self.n_routing = n_routing\n",
    "        self.kernel_initializer = initializers.get('he_normal')\n",
    "        self.bias_initializer = initializers.get('zeros')\n",
    "\n",
    "    def build(self, input_shape): # input_shape is a 4D tensor\n",
    "        _, self.input_n_capsule, self.input_n_vector, *_ = input_shape\n",
    "        self.W = self.add_weight(shape=[self.input_n_capsule, self.n_capsule, self.input_n_vector, self.n_vector], initializer=self.kernel_initializer, name='W')\n",
    "        self.bias = self.add_weight(shape=[1, self.input_n_capsule, self.n_capsule, 1, 1], initializer=self.bias_initializer, name='bias', trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_expand = tf.expand_dims(tf.expand_dims(inputs, 2), 2)\n",
    "        input_tiled = tf.tile(input_expand, [1, 1, self.n_capsule, 1, 1])\n",
    "        input_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]), elems=input_tiled, initializer=K.zeros( [self.input_n_capsule, self.n_capsule, 1, self.n_vector]))\n",
    "        for i in range(self.n_routing): # routing\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            outputs = squash(tf.reduce_sum( c * input_hat, axis=1, keep_dims=True))\n",
    "            if i != self.n_routing - 1:\n",
    "                self.bias += tf.reduce_sum(input_hat * outputs, axis=-1, keep_dims=True)\n",
    "        return tf.reshape(outputs, [-1, self.n_capsule, self.n_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # output current layer capsules\n",
    "        return (None, self.n_capsule, self.n_vector)\n",
    "\n",
    "#\n",
    "# This layer takes 'n_class' arrays as input, outputs an array of size 'n_class',\n",
    "# each eleemnt in the output array represent the possibility,\n",
    "# i.e., the last layer in Figure 2.\n",
    "#\n",
    "class LengthLayer(Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), axis=-1, keep_dims=False))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *output_shape, _ = input_shape\n",
    "        return tuple(output_shape)\n",
    "\n",
    "\n",
    "#\n",
    "# margin loss is employed to measure the accuracy of the capsule net,\n",
    "# in the code below, mean absolute error is used to measure the accuracy of the reconstructed image\n",
    "#\n",
    "def margin_loss(y_ground_truth, y_prediction):\n",
    "    _m_plus = 0.9\n",
    "    _m_minus = 0.1\n",
    "    _lambda = 0.5\n",
    "    L = y_ground_truth * tf.square(tf.maximum(0., _m_plus - y_prediction)) + _lambda * ( 1 - y_ground_truth) * tf.square(tf.maximum(0., y_prediction - _m_minus))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # preprocess MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    X = np.concatenate((x_train, x_test), axis=0)\n",
    "    Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "    # make model\n",
    "    input_shape = [28, 28, 1]\n",
    "    n_class = 10\n",
    "    n_routing = 3\n",
    "\n",
    "    # encoder\n",
    "    x = Input(shape=input_shape)\n",
    "    conv1 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    primary_capsule = PrimaryCapsule( n_vector=8, n_channel=32, n_kernel_size=9, n_stride=2)(conv1)\n",
    "    digit_capsule = CapsuleLayer( n_capsule=n_class, n_vec=16, n_routing=n_routing, name='digit_capsule')(primary_capsule)\n",
    "    output_capsule = LengthLayer(name='output_capsule')(digit_capsule)\n",
    "\n",
    "    # decoder\n",
    "    mask_input = Input(shape=(n_class, ))\n",
    "    mask = MaskingLayer()([digit_capsule, mask_input])  # two inputs\n",
    "    dec = Dense(512, activation='relu')(mask)\n",
    "    dec = Dense(1024, activation='relu')(dec)\n",
    "    dec = Dense(784, activation='sigmoid')(dec)\n",
    "    dec = Reshape(input_shape)(dec)\n",
    "\n",
    "    model = Model([x, mask_input], [output_capsule, dec])\n",
    "    plot_model(model, 'capsule.png', show_shapes=True, rankdir='TB')\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss=[ margin_loss, 'mae' ], metrics=[ margin_loss, 'mae'])\n",
    "\n",
    "    # train capsule model\n",
    "    model.fit([X, Y], [Y, X], batch_size=128, epochs=50, validation_split=0.2)\n",
    "    #model.save_weights('capsule_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://search.daum.net/search?nil_suggest=sugsch&w=tot&DA=GIQ&sq=Boilerplate+code&o=1&sugo=1&q=Boilerplate+code\n",
    "import gc\n",
    "import os\n",
    "import nltk\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, words_dict):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        if hasattr(sentence, \"decode\"):\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        result = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = len(words_dict)\n",
    "            word_index = words_dict[word]\n",
    "            result.append(word_index)\n",
    "        tokenized_sentences.append(result)\n",
    "    return tokenized_sentences, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_list(file_path):\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    f = open(file_path)\n",
    "\n",
    "    for index, line in enumerate(f):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            continue\n",
    "        embedding_list.append(coefs)\n",
    "        embedding_word_dict[word] = len(embedding_word_dict)\n",
    "    f.close()\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n",
    "    cleared_embedding_list = []\n",
    "    cleared_embedding_word_dict = {}\n",
    "\n",
    "    for word in words_dict:\n",
    "        if word not in embedding_word_dict:\n",
    "            continue\n",
    "        word_id = embedding_word_dict[word]\n",
    "        row = embedding_list[word_id]\n",
    "        cleared_embedding_list.append(row)\n",
    "        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n",
    "\n",
    "    return cleared_embedding_list, cleared_embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokenized_sentences, words_list, embedding_word_dict, sentences_length):\n",
    "    words_train = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            word = words_list[word_index]\n",
    "            word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n",
    "            current_words.append(word_id)\n",
    "\n",
    "        if len(current_words) >= sentences_length:\n",
    "            current_words = current_words[:sentences_length]\n",
    "        else:\n",
    "            current_words += [len(embedding_word_dict) - 1] * (sentences_length - len(current_words))\n",
    "        words_train.append(current_words)\n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.3\n",
    "rate_drop_dense = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, sequence_length, dropout_rate, recurrent_units, dense_size):\n",
    "    input1 = Input(shape=(sequence_length,))\n",
    "    embed_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(1, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    num_labels = train_y.shape[1]\n",
    "    patience = 5\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(num_labels):\n",
    "            loss = log_loss(val_y[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_labels\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == patience:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(X, y, X_test, fold_count, batch_size, get_model_func):\n",
    "    print(\"=\"*75)\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    result_path = \"predictions\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = np.array(X[fold_start:fold_end])\n",
    "        val_y = np.array(y[fold_start:fold_end])\n",
    "\n",
    "        model = _train_model(get_model_func(), batch_size, train_x, train_y, val_x, val_y)\n",
    "        train_predicts_path = os.path.join(result_path, \"train_predicts{0}.npy\".format(fold_id))\n",
    "        test_predicts_path = os.path.join(result_path, \"test_predicts{0}.npy\".format(fold_id))\n",
    "        train_predicts = model.predict(X, batch_size=512, verbose=1)\n",
    "        test_predicts = model.predict(X_test, batch_size=512, verbose=1)\n",
    "        np.save(train_predicts_path, train_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"../input/donorschooseorg-preprocessed-data/train_preprocessed.csv\"\n",
    "train_file_path = \"../input/donorschooseorg-preprocessed-data/train_small.csv\"\n",
    "\n",
    "# test_file_path = \"../input/donorschooseorg-preprocessed-data/test_preprocessed.csv\"\n",
    "test_file_path = \"../input/donorschooseorg-preprocessed-data/test_small.csv\"\n",
    "\n",
    "# embedding_path = \"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\n",
    "embedding_path = \"../input/donorschooseorg-preprocessed-data/embeddings_small.vec\"\n",
    "\n",
    "batch_size = 128 # 256\n",
    "recurrent_units = 16 # 64\n",
    "dropout_rate = 0.3 \n",
    "dense_size = 8 # 32\n",
    "sentences_length = 10 # 300\n",
    "fold_count = 2 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcdl0tf3111",
   "language": "python",
   "name": "wcdl0tf3111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
